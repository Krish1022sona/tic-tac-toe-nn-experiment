{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 393.7922, LR = 0.010000\n",
      "Epoch 1000: Loss = 340.4644, LR = 0.010000\n",
      "Epoch 2000: Loss = 326.3074, LR = 0.010000\n",
      "Epoch 3000: Loss = 315.6334, LR = 0.010000\n",
      "Epoch 4000: Loss = 306.6355, LR = 0.010000\n",
      "Epoch 5000: Loss = 297.7362, LR = 0.010000\n",
      "Epoch 6000: Loss = 289.5123, LR = 0.010000\n",
      "Epoch 7000: Loss = 281.9873, LR = 0.010000\n",
      "Epoch 8000: Loss = 275.5842, LR = 0.010000\n",
      "Epoch 9000: Loss = 270.0065, LR = 0.010000\n",
      "Epoch 10000: Loss = 265.2296, LR = 0.010000\n",
      "Epoch 11000: Loss = 260.9025, LR = 0.010000\n",
      "Epoch 12000: Loss = 256.8460, LR = 0.010000\n",
      "Epoch 13000: Loss = 253.7095, LR = 0.010000\n",
      "Epoch 14000: Loss = 250.9662, LR = 0.010000\n",
      "Epoch 15000: Loss = 248.5295, LR = 0.010000\n",
      "Epoch 16000: Loss = 246.2315, LR = 0.010000\n",
      "Epoch 17000: Loss = 243.9883, LR = 0.010000\n",
      "Epoch 18000: Loss = 241.8652, LR = 0.010000\n",
      "Epoch 19000: Loss = 239.5863, LR = 0.010000\n",
      "Epoch 20000: Loss = 237.3756, LR = 0.010000\n",
      "Epoch 21000: Loss = 235.1400, LR = 0.010000\n",
      "Epoch 22000: Loss = 232.8060, LR = 0.010000\n",
      "Epoch 23000: Loss = 230.7473, LR = 0.010000\n",
      "Epoch 24000: Loss = 228.7875, LR = 0.010000\n",
      "Epoch 25000: Loss = 226.3815, LR = 0.010000\n",
      "Epoch 26000: Loss = 223.7383, LR = 0.010000\n",
      "Epoch 27000: Loss = 221.2759, LR = 0.010000\n",
      "Epoch 28000: Loss = 218.9514, LR = 0.010000\n",
      "Epoch 29000: Loss = 217.0213, LR = 0.010000\n",
      "Epoch 30000: Loss = 215.3351, LR = 0.010000\n",
      "Epoch 31000: Loss = 213.7732, LR = 0.010000\n",
      "Epoch 32000: Loss = 212.3752, LR = 0.010000\n",
      "Epoch 33000: Loss = 210.9036, LR = 0.010000\n",
      "Epoch 34000: Loss = 209.3990, LR = 0.010000\n",
      "Epoch 35000: Loss = 207.9275, LR = 0.010000\n",
      "Epoch 36000: Loss = 206.5608, LR = 0.010000\n",
      "Epoch 37000: Loss = 205.0666, LR = 0.010000\n",
      "Epoch 38000: Loss = 203.2596, LR = 0.010000\n",
      "Epoch 39000: Loss = 201.8757, LR = 0.010000\n",
      "Epoch 40000: Loss = 200.7019, LR = 0.009409\n",
      "Epoch 41000: Loss = 199.5775, LR = 0.007374\n",
      "Epoch 42000: Loss = 198.8231, LR = 0.004262\n",
      "Epoch 43000: Loss = 198.4701, LR = 0.001873\n",
      "Epoch 44000: Loss = 198.3419, LR = 0.000589\n",
      "Epoch 45000: Loss = 198.2938, LR = 0.000259\n",
      "Epoch 46000: Loss = 198.2766, LR = 0.000095\n",
      "Epoch 47000: Loss = 198.2705, LR = 0.000024\n",
      "Epoch 48000: Loss = 198.2690, LR = 0.000005\n",
      "Epoch 49000: Loss = 198.2686, LR = 0.000001\n",
      "Epoch 50000: Loss = 198.2686, LR = 0.000000\n",
      "Epoch 51000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 52000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 53000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 54000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 55000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 56000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 57000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 58000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 59000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 60000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 61000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 62000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 63000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 64000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 65000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 66000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 67000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 68000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 69000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 70000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 71000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 72000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 73000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 74000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 75000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 76000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 77000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 78000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 79000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 80000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 81000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 82000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 83000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 84000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 85000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 86000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 87000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 88000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 89000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 90000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 91000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 92000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 93000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 94000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 95000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 96000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 97000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 98000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 99000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 100000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 101000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 102000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 103000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 104000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 105000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 106000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 107000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 108000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 109000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 110000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 111000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 112000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 113000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 114000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 115000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 116000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 117000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 118000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 119000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 120000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 121000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 122000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 123000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 124000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 125000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 126000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 127000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 128000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 129000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 130000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 131000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 132000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 133000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 134000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 135000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 136000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 137000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 138000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 139000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 140000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 141000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 142000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 143000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 144000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 145000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 146000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 147000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 148000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 149000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 150000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 151000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 152000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 153000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 154000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 155000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 156000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 157000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 158000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 159000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 160000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 161000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 162000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 163000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 164000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 165000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 166000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 167000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 168000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 169000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 170000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 171000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 172000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 173000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 174000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 175000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 176000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 177000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 178000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 179000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 180000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 181000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 182000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 183000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 184000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 185000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 186000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 187000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 188000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 189000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 190000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 191000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 192000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 193000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 194000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 195000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 196000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 197000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 198000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 199000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 200000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 201000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 202000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 203000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 204000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 205000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 206000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 207000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 208000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 209000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 210000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 211000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 212000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 213000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 214000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 215000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 216000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 217000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 218000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 219000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 220000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 221000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 222000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 223000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 224000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 225000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 226000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 227000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 228000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 229000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 230000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 231000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 232000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 233000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 234000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 235000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 236000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 237000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 238000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 239000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 240000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 241000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 242000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 243000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 244000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 245000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 246000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 247000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 248000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 249000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 250000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 251000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 252000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 253000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 254000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 255000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 256000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 257000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 258000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 259000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 260000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 261000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 262000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 263000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 264000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 265000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 266000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 267000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 268000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 269000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 270000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 271000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 272000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 273000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 274000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 275000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 276000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 277000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 278000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 279000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 280000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 281000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 282000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 283000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 284000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 285000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 286000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 287000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 288000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 289000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 290000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 291000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 292000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 293000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 294000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 295000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 296000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 297000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 298000: Loss = 198.2685, LR = 0.000000\n",
      "Epoch 299000: Loss = 198.2685, LR = 0.000000\n",
      "198.2685162896121\n",
      "\n",
      "[0.23523712905702596, 0.07202485883076652, 0.5343692380948785, 0.04108318303724416, 0.009582075566347492, 0.08949441758178989, 0.016185465207419224, 2.482220661743857e-08, 0.0020236078023216633]\n"
     ]
    }
   ],
   "source": [
    "import neural_network\n",
    "\n",
    "test_data = neural_network.data.comprehensive_test_data\n",
    "\n",
    "# Corresponding optimal moves (1-9 positions)\n",
    "output_data = neural_network.data.comprehensive_output_data\n",
    "\n",
    "nn = neural_network.NN(9, 9, 1, 9)\n",
    "\n",
    "# for i in range(9):\n",
    "#     print(nn.hidden_layers[0][i].weights, nn.output_layer[i].bias)\n",
    "learning_rate = 0.01\n",
    "prev_loss = float('inf')\n",
    "# 50,000 for best result with decay 0.97 lr=0.01 structure, 9->9->9\n",
    "for i in range(50000): # Icreasing will not affect it\n",
    "    # Mini-batch training of size `batch_size`\n",
    "    # batch_size = 32\n",
    "    # indices = neural_network.random.sample(range(len(test_data)), batch_size)\n",
    "    # batch_inputs = [test_data[i] for i in indices]\n",
    "    # batch_outputs = [output_data[i] for i in indices]\n",
    "    # nn.train(batch_inputs, batch_outputs, learning_rate)\n",
    "\n",
    "    # full data training\n",
    "    nn.train(test_data, output_data, learning_rate)\n",
    "    if nn.loss() < prev_loss:\n",
    "        prev_loss = nn.loss()\n",
    "    else:\n",
    "        learning_rate *= 0.97\n",
    "\n",
    "    # print(nn.loss())\n",
    "    # if i%100 == 0: print(nn.loss())\n",
    "    # Decrease learning rate over time\n",
    "    # if i % 3000 == 0 and i > 0:\n",
    "    #     learning_rate *= 0.95\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Epoch {i}: Loss = {nn.loss():.4f}, LR = {learning_rate:.6f}\")\n",
    "        # nn.predict([1, 1, 0, -1, -1, 0, 0, 0, 0])\n",
    "        # print([neuron.value for neuron in nn.output_layer])\n",
    "        # print()\n",
    "\n",
    "print(nn.loss())\n",
    "print()\n",
    "nn.predict([1, 1, 0, -1, -1, 0, 0, 0, 0])\n",
    "print([neuron.value for neuron in nn.output_layer])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Tic Tac Toe AI\n",
      "1-> New Game\n",
      "2-> Quit\n",
      "Choose your Turn\n",
      "1-> X [Goes First]\n",
      "2-> O [Goes Second]\n",
      "  |  | \n",
      "--------\n",
      "  |  | \n",
      "--------\n",
      "  |  | \n",
      "\n",
      " Player 1\n",
      "\n",
      "X |  | \n",
      "--------\n",
      "  |  | \n",
      "--------\n",
      "  |  | \n",
      "\n",
      "X |  | \n",
      "--------\n",
      "  |O | \n",
      "--------\n",
      "  |  | \n",
      "\n",
      " Player 1\n",
      "\n",
      "X |  | \n",
      "--------\n",
      "X |O | \n",
      "--------\n",
      "  |  | \n",
      "\n",
      "X |  | \n",
      "--------\n",
      "X |O | \n",
      "--------\n",
      "O |  | \n",
      "\n",
      " Player 1\n",
      "\n",
      "X |  | \n",
      "--------\n",
      "X |O | \n",
      "--------\n",
      "O |X | \n",
      "\n",
      "X |  | \n",
      "--------\n",
      "X |O | \n",
      "--------\n",
      "O |X |O\n",
      "\n",
      " Player 1\n",
      "\n",
      "X |  |X\n",
      "--------\n",
      "X |O | \n",
      "--------\n",
      "O |X |O\n",
      "\n",
      "X |  |X\n",
      "--------\n",
      "X |O |O\n",
      "--------\n",
      "O |X |O\n",
      "\n",
      " Player 1\n",
      "\n",
      "X |X |X\n",
      "--------\n",
      "X |O |O\n",
      "--------\n",
      "O |X |O\n",
      "\n",
      "You WON\n",
      "\n",
      "Welcome to Tic Tac Toe AI\n",
      "1-> New Game\n",
      "2-> Quit\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mneural_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game_AI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\Krishna ka Folder\\The Coding Folder\\coding ki duniya\\Machine Learning\\Neural Networks\\Tic Tac Toe\\try 1\\neural_network.py:151\u001b[0m, in \u001b[0;36mstart_game_AI\u001b[1;34m(nn)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-> New Game\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2-> Quit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 151\u001b[0m ch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your Choice: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(ch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThanks For Playing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "print()\n",
    "neural_network.start_game_AI(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer Neuron 0 Weights: [0.36821545738305134, 1.5505063690892997, -1.1398227977130708, -0.3389725174746923, -0.7592326123233472, 0.3860022585576313, 1.0551948069036374, -0.4957093140229032, 2.249638852724647]\n",
      "\n",
      "Input Layer Neuron 1 Weights: [-1.3129919877687612, 2.765801424730374, 1.0186685446171877, 0.9335421167133688, 1.938828018523383, 0.9114204106300339, 2.1022971910754418, 0.3410553176685509, 0.30997925396221815]\n",
      "\n",
      "Input Layer Neuron 2 Weights: [-1.1908375005244698, 0.8187122443571786, 0.6323138448055684, 0.7791622941957201, 0.9074187563474648, 0.411836421024585, -0.1874238378714373, -0.34014755728362384, -0.6938318050679385]\n",
      "\n",
      "Input Layer Neuron 3 Weights: [1.6509231624551461, 0.5829461699070441, -2.0975083969444284, 0.7066652246149406, 0.08348005361101103, 0.8969191189853861, -0.7544305371138231, 1.500771962481971, 0.43628379846670173]\n",
      "\n",
      "Input Layer Neuron 4 Weights: [0.9144922476890437, -0.9977963039572976, 3.458939910981765, -1.8951967510738084, 1.4925857319385436, 1.9782849336155213, -0.1874238378714446, 2.0187044714507056, 0.7670216985263173]\n",
      "\n",
      "Input Layer Neuron 5 Weights: [-1.2268008593934454, 1.7351186599721409, -0.18851291321688574, 1.0335382290558877, 0.1714857531990505, -1.4411622647258842, 0.4887109298213644, -0.18378828364335398, -1.3707569991938566]\n",
      "\n",
      "Input Layer Neuron 6 Weights: [-1.3737600306664945, 0.03601957526403253, -1.4293736730984903, 1.6359537472116714, -1.7486234731248398, 0.4118364210245848, -0.9704101391864609, -1.8926506035401311, -1.3347247115591796]\n",
      "\n",
      "Input Layer Neuron 7 Weights: [-1.4097233895354702, -0.39658281128389594, 0.5536462486754754, 0.06039755654703419, 0.16258516515527618, -0.9114204106300321, 0.11334043191837705, 0.9440932112288227, -0.5892909111675317]\n",
      "\n",
      "Input Layer Neuron 8 Weights: [0.47971137423773486, 0.5326916746253381, -1.8965862583522255, 0.7971024459184862, -0.6857711475850157, -0.14281057762131205, -2.454154222823436, 0.40766288684783225, -0.8921908331115739]\n",
      "\n",
      "Hidden Layer Neuron 0 Weights: [1.0746067380654687, -1.7119200704489606, -2.0159927130540782, 1.0362183227350208, 1.5293016019050654, 0.6628639233896034, 0.04988559740771285, -0.7698675703340805, -0.46644418574705937]\n",
      "Hidden Layer Neuron 0 Bias: -0.8226220431414186\n",
      "\n",
      "Hidden Layer Neuron 1 Weights: [0.011199750647053074, 1.5603686297697845, 0.8085711452244423, 0.0668778405472624, -0.8361717501685041, -1.2489888053623661, 0.22699054725499954, -2.9057576968531476, 1.0008805978485913]\n",
      "Hidden Layer Neuron 1 Bias: 0.41660120179056015\n",
      "\n",
      "Hidden Layer Neuron 2 Weights: [1.6418844263161345, 1.3719624062588425, -0.1813839489317372, 0.4521178745254334, -4.128380557113423, -1.5416496053676267, 1.3879605328628006, -0.2670236016284188, 0.02683321319102892]\n",
      "Hidden Layer Neuron 2 Bias: -1.7721366425186396\n",
      "\n",
      "Hidden Layer Neuron 3 Weights: [0.32186580140859694, -0.5569189700591577, 0.05872756717432179, 0.6491152733004727, -2.533535983707444, 0.4817439657486117, 0.9116539772669102, 1.0732909248678673, 0.6015389700132193]\n",
      "Hidden Layer Neuron 3 Bias: -0.326873004440947\n",
      "\n",
      "Hidden Layer Neuron 4 Weights: [-0.8651525844451885, -0.7252163254268785, -1.0736528122005695, 0.28751789231763564, 1.8661592674261769, -1.178639661090113, -0.16334266866333821, 1.6284836368598439, 1.17015335853518]\n",
      "Hidden Layer Neuron 4 Bias: 0.5342479861728788\n",
      "\n",
      "Hidden Layer Neuron 5 Weights: [-0.020222999182683277, -1.3193320901740544, 1.050841027527255, -2.051997973129947, 0.9619968533299602, -0.04805516094355576, 0.3521209142052384, 1.0565443990877172, 0.7520060770700695]\n",
      "Hidden Layer Neuron 5 Bias: 0.7978386795822159\n",
      "\n",
      "Hidden Layer Neuron 6 Weights: [0.3745942411105338, -1.698840632012208, -0.5144543868028384, -0.12435451991645821, 1.4581095892058697, 1.6421723072792955, -0.8090532455301502, -0.2065789063906449, -2.1375968440716373]\n",
      "Hidden Layer Neuron 6 Bias: 0.3007642697898143\n",
      "\n",
      "Hidden Layer Neuron 7 Weights: [-0.18111513225917608, 1.5942234550247132, 1.6407687219184617, 0.5105372435341301, -1.2430905114354547, 0.8954202164122893, -0.18444178394246996, -1.4378068571487743, -0.31199478386639035]\n",
      "Hidden Layer Neuron 7 Bias: 0.006911092343170374\n",
      "\n",
      "Hidden Layer Neuron 8 Weights: [-1.000577952118506, 0.9172978009607528, 0.8753071618667164, -0.19345811767706367, -2.11396429694124, 0.13668001447914038, 0.7724916155529518, 0.6612395980599457, 1.2786571565951632]\n",
      "Hidden Layer Neuron 8 Bias: -0.45704244456409915\n",
      "\n",
      "Output Layer Neuron 0 Weights: [-0.2293856810762971, -0.5219159435873679, -0.5536940634529058, -0.039688957247400625, -0.7012011276652995, 0.24918615887785722, 0.14586009330745509, 0.6229491382690058, -0.6235438394394186]\n",
      "Output Layer Neuron 0 Bias: 0.7378962013578277\n",
      "\n",
      "Output Layer Neuron 1 Weights: [-0.3670386040581284, -0.6075870798333028, 0.7338810868876253, 0.7191437690742426, 0.6908446045635646, 0.5597485380895921, 0.6747029806387144, -0.10766212623178706, 0.5622269837888934]\n",
      "Output Layer Neuron 1 Bias: 0.23865063721378002\n",
      "\n",
      "Output Layer Neuron 2 Weights: [-0.3507745362843601, -0.1815482889905825, 0.047368928701599144, 0.45303607076752894, -0.06902724888783551, -0.035013989371754595, -0.529580634398851, -0.4091250672295939, 0.13731278399288793]\n",
      "Output Layer Neuron 2 Bias: 0.09061408937139954\n",
      "\n",
      "Output Layer Neuron 3 Weights: [0.685312495924578, 0.6787312716292773, 0.7619669834341385, 0.36328334336556156, 0.11901850162823435, 0.6766389992596009, -0.654906752244576, 0.5837877065578128, 0.4641870662714105]\n",
      "Output Layer Neuron 3 Bias: -0.45946749011876387\n",
      "\n",
      "Output Layer Neuron 4 Weights: [-0.052243625955918804, -0.5405786877623113, -0.1037137978530358, -0.6022084504180849, 0.5646706474742598, 0.7743004456538112, 0.09576029640157302, -0.25840713490779976, -0.5691592702232049]\n",
      "Output Layer Neuron 4 Bias: 1.9147112143570397\n",
      "\n",
      "Output Layer Neuron 5 Weights: [0.2572600914974086, 0.09359634973202635, -0.15972744235224579, 0.44171147299783287, -0.5811749437230331, 0.33897736574208803, -0.023726006790061094, 0.5480273893692473, 0.2530012276591673]\n",
      "Output Layer Neuron 5 Bias: -0.5311879481908989\n",
      "\n",
      "Output Layer Neuron 6 Weights: [-0.6073029403193604, -0.7765744797059283, 0.20288115456345768, 0.2042281179598162, -0.6011748913580597, 0.3680855164211576, -0.14242954062771673, 0.16876256445348725, 0.5847020767111524]\n",
      "Output Layer Neuron 6 Bias: -0.3912717546739817\n",
      "\n",
      "Output Layer Neuron 7 Weights: [-0.5957690916365058, 0.40533154614267985, 0.30846452493046295, 0.8057219115021659, -0.20065444821130474, 0.29507532868440145, 0.3448687175277785, -0.2972853473466991, 0.6418050990692901]\n",
      "Output Layer Neuron 7 Bias: -0.685685827793107\n",
      "\n",
      "Output Layer Neuron 8 Weights: [-0.022841649853715618, 0.13089090001643156, -0.5138617082039552, -0.30290956467970487, -0.585521550975939, 0.47570000577821014, 0.13601620006486292, 0.8025079616822173, -0.34671827866743693]\n",
      "Output Layer Neuron 8 Bias: -0.7958714382849067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, neuron in enumerate(nn.input_layer):\n",
    "    print(f\"Input Layer Neuron {i} Weights: {neuron.weights}\")\n",
    "    print()\n",
    "\n",
    "for i, neuron in enumerate(nn.hidden_layers[0]):\n",
    "    print(f\"Hidden Layer Neuron {i} Weights: {neuron.weights}\")\n",
    "    print(f\"Hidden Layer Neuron {i} Bias: {neuron.bias}\")\n",
    "    print()\n",
    "\n",
    "for i, neuron in enumerate(nn.output_layer):\n",
    "    print(f\"Output Layer Neuron {i} Weights: {neuron.weights}\")\n",
    "    print(f\"Output Layer Neuron {i} Bias: {neuron.bias}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
